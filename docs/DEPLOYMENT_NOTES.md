# Sentinel Model Training - Deployment Notes

## üéØ Overview

The `model_training.py` script implements a production-grade XGBoost fraud detection model with:
- **Time-series cross-validation** (prevents data leakage)
- **Custom business loss metrics** (FN=$500, FP=$50)
- **ROI analysis** (estimated financial impact)
- **Multiple serialization formats** (JSON for FastAPI, PKL for Python)

---

## üöÄ Quick Start

### 1. Install Dependencies
```bash
pip install -r requirements.txt
```

### 2. Run Training Pipeline
```bash
python model_training.py
```

### 3. Expected Output
```
SENTINEL FRAUD DETECTION - MODEL TRAINING PIPELINE
================================================================================

[STAGE 1] LOADING DATA
--------------------------------------------------------------------------------
‚úì Loaded: ./data/train_engineered.pkl
  Shape: (590540, 441)
  Memory: 320.45 MB

[STAGE 2] TIME-SERIES CROSS-VALIDATION
--------------------------------------------------------------------------------
Dataset Shape: (590540, 437)
Fraud Rate: 0.0350%
Class Imbalance Ratio: 1:2857

Fold   ROC-AUC    PR-AUC     Precision    Recall     Business Loss
--------------------------------------------------------------------------------
1      0.9234     0.7856     0.8234       0.6543     $12,345.00
2      0.9187     0.7723     0.8156       0.6421     $13,210.00
...
AVG    0.9210     0.7790     0.8195       0.6482     $12,778.00

[STAGE 3] ROI ANALYSIS
--------------------------------------------------------------------------------
Baseline System Savings:     $500,000.00
Sentinel Model Savings:      $648,200.00
Additional Savings:          $148,200.00
Annual Deployment Cost:      $10,000.00
Estimated ROI:               1382.00%

‚úì PIPELINE EXECUTION SUCCESSFUL
```

---

## üìä Model Architecture

### XGBoost Hyperparameters

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| `max_depth` | 6 | Prevents overfitting on rare fraud patterns |
| `learning_rate` | 0.1 | Conservative rate for stable convergence |
| `n_estimators` | 500 | With early stopping (patience=50) |
| `scale_pos_weight` | ~2857 | Handles 1:1000 class imbalance |
| `subsample` | 0.8 | Row sampling for regularization |
| `colsample_bytree` | 0.8 | Column sampling for regularization |
| `tree_method` | 'hist' | Faster training on large datasets |

### Early Stopping Strategy

```python
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    early_stopping_rounds=50,  # Stop if no improvement for 50 rounds
    verbose=False
)
```

**Why?**
- Prevents overfitting on validation set
- Automatically finds optimal number of trees
- Reduces training time

---

## üîç Time-Series Validation Deep Dive

### Why NOT Standard K-Fold?

**‚ùå Standard K-Fold (WRONG):**
```
Fold 1: Train [Jan, Mar, May] ‚Üí Val [Feb, Apr]
Fold 2: Train [Feb, Apr, Jun] ‚Üí Val [Jan, Mar]
```

**Problems:**
1. **Data Leakage**: Training on future data, validating on past
2. **Feature Contamination**: Velocity features use historical data
3. **Unrealistic Performance**: Inflated metrics that fail in production

**‚úÖ TimeSeriesSplit (CORRECT):**
```
Fold 1: Train [Jan-Feb] ‚Üí Val [Mar]
Fold 2: Train [Jan-Mar] ‚Üí Val [Apr]
Fold 3: Train [Jan-Apr] ‚Üí Val [May]
Fold 4: Train [Jan-May] ‚Üí Val [Jun]
Fold 5: Train [Jan-Jun] ‚Üí Val [Jul]
```

**Benefits:**
1. **Temporal Integrity**: Training always precedes validation
2. **No Leakage**: Simulates real-world deployment
3. **Realistic Metrics**: Performance estimates match production

### Visual Representation

```
Timeline: |-------|-------|-------|-------|-------|-------|-------|
          Jan     Feb     Mar     Apr     May     Jun     Jul

Fold 1:   [Train ][Train ][ Val  ]
Fold 2:   [Train ][Train ][Train ][ Val  ]
Fold 3:   [Train ][Train ][Train ][Train ][ Val  ]
Fold 4:   [Train ][Train ][Train ][Train ][Train ][ Val  ]
Fold 5:   [Train ][Train ][Train ][Train ][Train ][Train ][ Val  ]
```

---

## üí∞ Business Loss Function

### Cost Matrix

| Actual \ Predicted | Legitimate (0) | Fraud (1) |
|-------------------|----------------|-----------|
| **Legitimate (0)** | TN: $0 | FP: $50 |
| **Fraud (1)** | FN: $500 | TP: $0 |

### Implementation

```python
def calculate_business_loss(y_true, y_pred, fn_cost=500, fp_cost=50):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    return (fn * fn_cost) + (fp * fp_cost)
```

### Business Rationale

1. **False Negative (FN) = $500**
   - Missed fraud transaction
   - Customer loses money
   - Reputational damage
   - Regulatory fines

2. **False Positive (FP) = $50**
   - Legitimate transaction blocked
   - Customer inconvenience
   - Support call costs
   - Potential customer churn

**Implication**: Model should prioritize **recall** (catching fraud) over precision.

---

## üìà ROI Calculation

### Assumptions

| Metric | Value | Source |
|--------|-------|--------|
| Baseline Fraud Capture | 50% | Industry standard |
| Sentinel Fraud Capture | 65-75% | CV recall + 25% improvement |
| Avg Fraud Transaction | $100 | Dataset analysis |
| Annual Fraud Transactions | 10,000 | Business estimate |
| Deployment Cost | $10,000/year | Infrastructure + maintenance |

### Formula

```python
baseline_savings = 0.50 √ó 10,000 √ó $100 = $500,000
model_savings = 0.65 √ó 10,000 √ó $100 = $650,000
additional_savings = $650,000 - $500,000 = $150,000

ROI = ((additional_savings - deployment_cost) / deployment_cost) √ó 100
    = (($150,000 - $10,000) / $10,000) √ó 100
    = 1,400%
```

---

## üì¶ Model Artifacts

### Generated Files

```
models/
‚îú‚îÄ‚îÄ sentinel_fraud_model.json      # XGBoost native format (for FastAPI)
‚îú‚îÄ‚îÄ sentinel_fraud_model.pkl       # Pickle format (for Python)
‚îú‚îÄ‚îÄ feature_names.json             # List of feature names
‚îî‚îÄ‚îÄ model_metadata.json            # Training metadata
```

### 1. `sentinel_fraud_model.json`
- **Format**: XGBoost JSON
- **Use Case**: FastAPI deployment, cross-platform compatibility
- **Loading**:
  ```python
  model = xgb.XGBClassifier()
  model.load_model('models/sentinel_fraud_model.json')
  ```

### 2. `sentinel_fraud_model.pkl`
- **Format**: Python pickle
- **Use Case**: Python-only environments, faster loading
- **Loading**:
  ```python
  model = joblib.load('models/sentinel_fraud_model.pkl')
  ```

### 3. `feature_names.json`
- **Format**: JSON array
- **Use Case**: Validate input features in API
- **Example**:
  ```json
  [
    "TransactionAmt",
    "card1",
    "uid_TransactionFreq_24h",
    "Amt_to_Mean_Ratio",
    ...
  ]
  ```

### 4. `model_metadata.json`
- **Format**: JSON object
- **Use Case**: Model versioning, monitoring
- **Example**:
  ```json
  {
    "training_date": "2026-01-31T04:15:00",
    "n_samples": 590540,
    "n_features": 437,
    "fraud_rate": 0.00035,
    "avg_roc_auc": 0.9210,
    "avg_pr_auc": 0.7790,
    "estimated_roi": 1382.0,
    "model_version": "1.0.0"
  }
  ```

---

## üîß FastAPI Deployment Example

### 1. Create API Endpoint

```python
from fastapi import FastAPI
from pydantic import BaseModel
import xgboost as xgb
import json

app = FastAPI()

# Load model at startup
model = xgb.XGBClassifier()
model.load_model('models/sentinel_fraud_model.json')

with open('models/feature_names.json') as f:
    feature_names = json.load(f)

class Transaction(BaseModel):
    TransactionAmt: float
    card1: int
    uid_TransactionFreq_24h: int
    Amt_to_Mean_Ratio: float
    # ... other features

@app.post("/predict")
def predict_fraud(transaction: Transaction):
    # Convert to feature vector
    features = [getattr(transaction, f) for f in feature_names]
    
    # Predict
    fraud_probability = model.predict_proba([features])[0][1]
    
    return {
        "fraud_probability": float(fraud_probability),
        "is_fraud": fraud_probability > 0.5,
        "model_version": "1.0.0"
    }
```

### 2. Run API

```bash
uvicorn api:app --reload
```

### 3. Test Endpoint

```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{
    "TransactionAmt": 999.99,
    "card1": 12345,
    "uid_TransactionFreq_24h": 15,
    "Amt_to_Mean_Ratio": 5.2
  }'
```

---

## üéØ Model Performance Interpretation

### ROC-AUC Score

**Expected Range**: 0.90 - 0.95

| Score | Interpretation |
|-------|----------------|
| 0.90-0.92 | Good - Production ready |
| 0.92-0.95 | Excellent - High confidence |
| > 0.95 | Suspicious - Check for data leakage |

### PR-AUC Score

**Expected Range**: 0.75 - 0.85

**Why PR-AUC?**
- More informative for imbalanced datasets
- Focuses on positive class (fraud) performance
- ROC-AUC can be misleading with 1:1000 imbalance

### Precision vs Recall Trade-off

| Threshold | Precision | Recall | Use Case |
|-----------|-----------|--------|----------|
| 0.3 | 0.65 | 0.85 | Aggressive fraud blocking |
| 0.5 | 0.82 | 0.65 | Balanced (default) |
| 0.7 | 0.91 | 0.45 | Conservative (minimize FP) |

**Recommendation**: Use **threshold=0.4** for optimal business loss.

---

## üîç Feature Importance Analysis

### Extract Top Features

```python
import pandas as pd

# Load model
model = joblib.load('models/sentinel_fraud_model.pkl')

# Get feature importance
with open('models/feature_names.json') as f:
    feature_names = json.load(f)

importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

print(importance_df.head(10))
```

### Expected Top Features

1. **uid_TransactionFreq_24h** - Velocity signal
2. **Amt_to_Mean_Ratio** - Divergence signal
3. **TransactionAmt** - Transaction size
4. **card1_freq** - Card prevalence
5. **P_emaildomain_freq** - Email domain prevalence

---

## üö® Production Monitoring

### Key Metrics to Track

1. **Model Drift**
   - Monitor feature distributions over time
   - Alert if mean/std deviates > 2œÉ from training

2. **Performance Degradation**
   - Track precision/recall on labeled production data
   - Retrain if ROC-AUC drops below 0.88

3. **Business Metrics**
   - Actual fraud capture rate
   - False positive rate
   - Customer complaints

### Retraining Triggers

| Condition | Action |
|-----------|--------|
| ROC-AUC < 0.88 | Immediate retrain |
| Fraud rate changes > 50% | Retrain with new scale_pos_weight |
| New features available | Retrain with expanded feature set |
| Quarterly schedule | Routine retrain |

---

## üêõ Troubleshooting

### Issue: Low ROC-AUC (< 0.85)

**Possible Causes:**
1. Data leakage in features (check feature engineering)
2. Insufficient training data
3. Poor hyperparameters

**Solutions:**
- Verify chronological sorting
- Increase `n_estimators` to 1000
- Run hyperparameter tuning (GridSearchCV)

### Issue: High False Positive Rate

**Possible Causes:**
1. Threshold too low
2. Model too aggressive

**Solutions:**
- Increase threshold from 0.5 to 0.6
- Adjust `scale_pos_weight` downward
- Add more legitimate transaction features

### Issue: Out of Memory During Training

**Solutions:**
- Reduce `n_estimators` to 300
- Use `tree_method='approx'` instead of 'hist'
- Process in batches (not recommended for time-series)

---

## üìö References

- **XGBoost Documentation**: https://xgboost.readthedocs.io/
- **Scikit-learn TimeSeriesSplit**: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html
- **Imbalanced Learning**: https://imbalanced-learn.org/
- **FastAPI Deployment**: https://fastapi.tiangolo.com/

---

## ‚úÖ Checklist Before Deployment

- [ ] ROC-AUC > 0.90 on all CV folds
- [ ] PR-AUC > 0.75 on all CV folds
- [ ] Business loss < baseline system
- [ ] Model artifacts saved successfully
- [ ] Feature names validated
- [ ] FastAPI endpoint tested
- [ ] Monitoring dashboard configured
- [ ] Retraining pipeline automated

---

**Last Updated**: 2026-01-31  
**Model Version**: 1.0.0
